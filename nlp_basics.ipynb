{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00cdc82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 0 - Hi,\n",
      "Token 1 - There\n",
      "Token 2 - !\n",
      "Token 3 - This\n",
      "Token 4 - is\n",
      "Token 5 - a\n",
      "Token 6 - notebook\n",
      "Token 7 - on\n",
      "Token 8 - Tokenization\n"
     ]
    }
   ],
   "source": [
    "doc = \"Hi, There ! This is a notebook on Tokenization\"\n",
    "for i,token in enumerate(doc.split(\" \")):\n",
    "    print(\"Token {} - {}\".format(i,token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3355c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Hi\n",
      "Token: ,\n",
      "Token: There\n",
      "Token: !\n",
      "Token: This\n",
      "Token: is\n",
      "Token: a\n",
      "Token: notebook\n",
      "Token: on\n",
      "Token: Tokenization\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#tokenizigng\n",
    "doc = nlp(\"Hi, There ! This is a notebook on Tokenization\")\n",
    "for token in doc:\n",
    "    print(\"Token: {}\".format(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "993bb7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-10 09:14:22.112574: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-10 09:14:22.407758: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-06-10 09:14:23.358046: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-10 09:14:23.358152: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-06-10 09:14:23.358159: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentences = [\n",
    "    \"Hi, This is our first Tokenizer Notebook\",\n",
    "    \"Glad to see you here.\",\n",
    "    \"What are you upto ?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "221de4f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'you': 1, 'hi': 2, 'this': 3, 'is': 4, 'our': 5, 'first': 6, 'tokenizer': 7, 'notebook': 8, 'glad': 9, 'to': 10, 'see': 11, 'here': 12, 'what': 13, 'are': 14, 'upto': 15}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=20)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_idx = tokenizer.word_index\n",
    "print(word_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9fd0e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3, 4, 5, 6, 7, 8]\n",
      "[9, 10, 11, 1, 12]\n",
      "[13, 14, 1, 15]\n"
     ]
    }
   ],
   "source": [
    "# converting each tokenized sentence into sequence\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "for seq in sequences:\n",
    "    print(seq) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f44ea409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 5 6 7 8]\n",
      "[ 9 10 11  1 12  0  0]\n",
      "[13 14  1 15  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "# to ensure that each sequence contains same number of tokens which are a primary need for any NN. We'll pad\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "for seq in padded_sequences:\n",
    "    print(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f2dd96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Word: program & Stem: program\n",
      "Original Word: programming & Stem: program\n",
      "Original Word: programmer & Stem: programm\n",
      "Original Word: programmed & Stem: program\n",
      "Original Word: programmatically & Stem: programmat\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "words = [\"program\",\"programming\",\"programmer\",\"programmed\",\"programmatically\"]\n",
    "\n",
    "for word in words:\n",
    "    print((\"Original Word: {} & Stem: {}\").format(word,stemmer.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "60a0e91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Word: Only & Stem: onli\n",
      "Original Word: a & Stem: a\n",
      "Original Word: troubled & Stem: troubl\n",
      "Original Word: programmer & Stem: programm\n",
      "Original Word: uses & Stem: use\n",
      "Original Word: troubling & Stem: troubl\n",
      "Original Word: methods & Stem: method\n",
      "Original Word: of & Stem: of\n",
      "Original Word: programming & Stem: program\n",
      "Original Word: to & Stem: to\n",
      "Original Word: write & Stem: write\n",
      "Original Word: a & Stem: a\n",
      "Original Word: better & Stem: better\n",
      "Original Word: program & Stem: program\n",
      "Original Word: with & Stem: with\n",
      "Original Word: which & Stem: which\n",
      "Original Word: others & Stem: other\n",
      "Original Word: are & Stem: are\n",
      "Original Word: not & Stem: not\n",
      "Original Word: troubled & Stem: troubl\n"
     ]
    }
   ],
   "source": [
    "# from sentences\n",
    "sentence = \"Only a troubled programmer uses troubling methods of programming to write a better program with which others are not troubled\"\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "for word in words:\n",
    "    print((\"Original Word: {} & Stem: {}\").format(word,stemmer.stem(word))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bdf48bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Word: program & Stem: program\n",
      "Original Word: programming & Stem: program\n",
      "Original Word: programmer & Stem: programm\n",
      "Original Word: programmed & Stem: program\n",
      "Original Word: programmatically & Stem: programmat\n"
     ]
    }
   ],
   "source": [
    "# similarly in place of porter stemmer we can use Snowball stemmer\n",
    "# let's compare the result of snowball stemmer.MARTIN_EXTENSIONS\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "snowball_stemmer = SnowballStemmer('english')\n",
    "words = [\"program\",\"programming\",\"programmer\",\"programmed\",\"programmatically\"]\n",
    "\n",
    "for word in words:\n",
    "    print((\"Original Word: {} & Stem: {}\").format(word,snowball_stemmer.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63494c04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Word: Only & Stem: onli\n",
      "Original Word: a & Stem: a\n",
      "Original Word: troubled & Stem: troubl\n",
      "Original Word: programmer & Stem: programm\n",
      "Original Word: uses & Stem: use\n",
      "Original Word: troubling & Stem: troubl\n",
      "Original Word: methods & Stem: method\n",
      "Original Word: of & Stem: of\n",
      "Original Word: programming & Stem: program\n",
      "Original Word: to & Stem: to\n",
      "Original Word: write & Stem: write\n",
      "Original Word: a & Stem: a\n",
      "Original Word: better & Stem: better\n",
      "Original Word: program & Stem: program\n",
      "Original Word: with & Stem: with\n",
      "Original Word: which & Stem: which\n",
      "Original Word: others & Stem: other\n",
      "Original Word: are & Stem: are\n",
      "Original Word: not & Stem: not\n",
      "Original Word: troubled & Stem: troubl\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Only a troubled programmer uses troubling methods of programming to write a better program with which others are not troubled\"\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "for word in words:\n",
    "    print((\"Original Word: {} & Stem: {}\").format(word,snowball_stemmer.stem(word))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99172fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Word: program & Stem: program\n",
      "Original Word: programming & Stem: program\n",
      "Original Word: programmer & Stem: program\n",
      "Original Word: programmed & Stem: program\n",
      "Original Word: programmatically & Stem: program\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "words = [\"program\",\"programming\",\"programmer\",\"programmed\",\"programmatically\"]\n",
    "\n",
    "for word in words:\n",
    "    print((\"Original Word: {} & Stem: {}\").format(word,lancaster_stemmer.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d29855f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Word: Only & Stem: on\n",
      "Original Word: a & Stem: a\n",
      "Original Word: troubled & Stem: troubl\n",
      "Original Word: programmer & Stem: program\n",
      "Original Word: uses & Stem: us\n",
      "Original Word: troubling & Stem: troubl\n",
      "Original Word: methods & Stem: method\n",
      "Original Word: of & Stem: of\n",
      "Original Word: programming & Stem: program\n",
      "Original Word: to & Stem: to\n",
      "Original Word: write & Stem: writ\n",
      "Original Word: a & Stem: a\n",
      "Original Word: better & Stem: bet\n",
      "Original Word: program & Stem: program\n",
      "Original Word: with & Stem: with\n",
      "Original Word: which & Stem: which\n",
      "Original Word: others & Stem: oth\n",
      "Original Word: are & Stem: ar\n",
      "Original Word: not & Stem: not\n",
      "Original Word: troubled & Stem: troubl\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Only a troubled programmer uses troubling methods of programming to write a better program with which others are not troubled\"\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "for word in words:\n",
    "    print((\"Original Word: {} & Stem: {}\").format(word,lancaster_stemmer.stem(word))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "257b9820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf18e2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The boy was going for a trip where he could say that he hiked, danced, sung, swam, surfed and cooked.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "601a4490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text - The and its lemma is the\n",
      "Text - boy and its lemma is boy\n",
      "Text - was and its lemma is be\n",
      "Text - going and its lemma is go\n",
      "Text - for and its lemma is for\n",
      "Text - a and its lemma is a\n",
      "Text - trip and its lemma is trip\n",
      "Text - where and its lemma is where\n",
      "Text - he and its lemma is he\n",
      "Text - could and its lemma is could\n",
      "Text - say and its lemma is say\n",
      "Text - that and its lemma is that\n",
      "Text - he and its lemma is he\n",
      "Text - hiked and its lemma is hike\n",
      "Text - , and its lemma is ,\n",
      "Text - danced and its lemma is danced\n",
      "Text - , and its lemma is ,\n",
      "Text - sung and its lemma is sung\n",
      "Text - , and its lemma is ,\n",
      "Text - swam and its lemma is swam\n",
      "Text - , and its lemma is ,\n",
      "Text - surfed and its lemma is surfed\n",
      "Text - and and its lemma is and\n",
      "Text - cooked and its lemma is cook\n",
      "Text - . and its lemma is .\n"
     ]
    }
   ],
   "source": [
    "output = nlp(text)\n",
    "for token in output:\n",
    "    print(\"Text - {} and its lemma is {}\".format(token.text, token.lemma_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d8fce802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lemminflect\n",
      "  Downloading lemminflect-0.2.3-py3-none-any.whl (769 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in ./Desktop/home/Desktop/envs/myenv/lib/python3.7/site-packages (from lemminflect) (1.21.6)\n",
      "Installing collected packages: lemminflect\n",
      "Successfully installed lemminflect-0.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install lemminflect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25af3041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text - He and its lemma is He\n",
      "Text - went and its lemma is go\n",
      "Text - to and its lemma is to\n",
      "Text - a and its lemma is a\n",
      "Text - trip and its lemma is trip\n",
      "Text - to and its lemma is to\n",
      "Text - later and its lemma is later\n",
      "Text - brag and its lemma is brag\n",
      "Text - that and its lemma is that\n",
      "Text - he and its lemma is he\n",
      "Text - hiked and its lemma is hike\n",
      "Text - , and its lemma is ,\n",
      "Text - swam and its lemma is swam\n",
      "Text - , and its lemma is ,\n",
      "Text - danced and its lemma is danced\n",
      "Text - , and its lemma is ,\n",
      "Text - sang and its lemma is sang\n",
      "Text - , and its lemma is ,\n",
      "Text - ran and its lemma is run\n",
      "Text - and and its lemma is and\n",
      "Text - cooked and its lemma is cook\n",
      "Text - . and its lemma is .\n"
     ]
    }
   ],
   "source": [
    "# let's evaluate an example\n",
    "\n",
    "import lemminflect\n",
    "doc = nlp('He went to a trip to later brag that he hiked, swam, danced, sang, ran and cooked.')\n",
    "\n",
    "for token in doc:\n",
    "    print(\"Text - {} and its lemma is {}\".format(token.text, token._.lemma()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0406d48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/maximus1/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f17954f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'could',\n",
       " 'did',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'indeed',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'made',\n",
       " 'make',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " \"n't\",\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'n‘t',\n",
       " 'n’t',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 're',\n",
       " 'really',\n",
       " 'regarding',\n",
       " 'same',\n",
       " 'say',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'under',\n",
       " 'unless',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'used',\n",
       " 'using',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " '‘d',\n",
       " '‘ll',\n",
       " '‘m',\n",
       " '‘re',\n",
       " '‘s',\n",
       " '‘ve',\n",
       " '’d',\n",
       " '’ll',\n",
       " '’m',\n",
       " '’re',\n",
       " '’s',\n",
       " '’ve'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "462a5659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check a demo.\n",
    "stop_words = nlp.Defaults.stop_words\n",
    "sentence = \"An example of stopwords for those demo purposes only\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff8a78c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An\n",
      "example\n",
      "of\n",
      "stopwords\n",
      "for\n",
      "those\n",
      "demo\n",
      "purposes\n",
      "only\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(sentence)\n",
    "filtered_words =[]\n",
    "for word in doc:\n",
    "    if word not in stop_words:\n",
    "        print(word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ecde213",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(\"This is an example to illustrate POS tagging using Spacy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e8bfd75f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is This and its POS is PRON\n",
      "Text is is and its POS is AUX\n",
      "Text is an and its POS is DET\n",
      "Text is example and its POS is NOUN\n",
      "Text is to and its POS is PART\n",
      "Text is illustrate and its POS is VERB\n",
      "Text is POS and its POS is PROPN\n",
      "Text is tagging and its POS is NOUN\n",
      "Text is using and its POS is VERB\n",
      "Text is Spacy and its POS is NOUN\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(\"Text is {} and its POS is {}\".format(token.text, token.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b67122c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is This and its root text is This, The dependency of root is nsubj\n",
      "The text is an example and its root text is example, The dependency of root is attr\n",
      "The text is POS tagging and its root text is tagging, The dependency of root is dobj\n",
      "The text is Spacy and its root text is Spacy, The dependency of root is dobj\n"
     ]
    }
   ],
   "source": [
    "# noun chunks are the noun plus the words describing the noun.\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(\"The text is {} and its root text is {}, The dependency of root is {}\".format(chunk.text,chunk.root.text, chunk.root.dep_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b5c236aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can render the relationships of various POS too\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f711288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"91351189e8d749c09190d40132e3ec52-0\" class=\"displacy\" width=\"1650\" height=\"377.0\" direction=\"ltr\" style=\"max-width: none; height: 377.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"287.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">This</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"287.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"210\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"210\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"287.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"370\">an</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"370\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"287.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">example</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"287.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"690\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"690\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"287.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">illustrate</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"287.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1010\">POS</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1010\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"287.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1170\">tagging</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1170\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"287.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1330\">using</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1330\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"287.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1490\">Spacy</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1490\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-91351189e8d749c09190d40132e3ec52-0-0\" stroke-width=\"2px\" d=\"M70,242.0 C70,162.0 200.0,162.0 200.0,242.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-91351189e8d749c09190d40132e3ec52-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,244.0 L62,232.0 78,232.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-91351189e8d749c09190d40132e3ec52-0-1\" stroke-width=\"2px\" d=\"M390,242.0 C390,162.0 520.0,162.0 520.0,242.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-91351189e8d749c09190d40132e3ec52-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M390,244.0 L382,232.0 398,232.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-91351189e8d749c09190d40132e3ec52-0-2\" stroke-width=\"2px\" d=\"M230,242.0 C230,82.0 525.0,82.0 525.0,242.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-91351189e8d749c09190d40132e3ec52-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M525.0,244.0 L533.0,232.0 517.0,232.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-91351189e8d749c09190d40132e3ec52-0-3\" stroke-width=\"2px\" d=\"M710,242.0 C710,162.0 840.0,162.0 840.0,242.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-91351189e8d749c09190d40132e3ec52-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M710,244.0 L702,232.0 718,232.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-91351189e8d749c09190d40132e3ec52-0-4\" stroke-width=\"2px\" d=\"M550,242.0 C550,82.0 845.0,82.0 845.0,242.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-91351189e8d749c09190d40132e3ec52-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">relcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M845.0,244.0 L853.0,232.0 837.0,232.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-91351189e8d749c09190d40132e3ec52-0-5\" stroke-width=\"2px\" d=\"M1030,242.0 C1030,162.0 1160.0,162.0 1160.0,242.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-91351189e8d749c09190d40132e3ec52-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1030,244.0 L1022,232.0 1038,232.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-91351189e8d749c09190d40132e3ec52-0-6\" stroke-width=\"2px\" d=\"M870,242.0 C870,82.0 1165.0,82.0 1165.0,242.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-91351189e8d749c09190d40132e3ec52-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1165.0,244.0 L1173.0,232.0 1157.0,232.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-91351189e8d749c09190d40132e3ec52-0-7\" stroke-width=\"2px\" d=\"M870,242.0 C870,2.0 1330.0,2.0 1330.0,242.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-91351189e8d749c09190d40132e3ec52-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1330.0,244.0 L1338.0,232.0 1322.0,232.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-91351189e8d749c09190d40132e3ec52-0-8\" stroke-width=\"2px\" d=\"M1350,242.0 C1350,162.0 1480.0,162.0 1480.0,242.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-91351189e8d749c09190d40132e3ec52-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1480.0,244.0 L1488.0,232.0 1472.0,232.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc,style='dep', jupyter=True, options={'distance':160})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a7b7a39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is first and its label is ORDINAL\n",
      "The text is NER and its label is ORG\n",
      "The text is VSCode and its label is ORG\n"
     ]
    }
   ],
   "source": [
    "text = \"Hi, This is our first example about NER on VSCode, Hope we cover the concepts in detail.\"\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(\"The text is {} and its label is {}\".format(ent.text, ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d1f0981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is Suraj and its label is ORG \n",
      "The text is India and its label is GPE \n",
      "The text is 4th April and its label is DATE \n"
     ]
    }
   ],
   "source": [
    "text2 = \"Suraj is a resident of India, born on 4th April.\"\n",
    "doc1 = nlp(text2)\n",
    "for ent in doc1.ents:\n",
    "        print(\"The text is {} and its label is {} \".format(ent.text, ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "511dead7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = \"This is Suraj and we want to show you some books on the topic- Gravitational Force\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e056ef12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is Suraj and its label is GPE - It's start is 8, End is 13 and It's start word index 2 + end word index is 3 \n",
      "The text is Gravitational Force and its label is FAC - It's start is 63, End is 82 and It's start word index 14 + end word index is 16 \n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(text3)\n",
    "for ent in doc2.ents:\n",
    "        print(\"The text is {} and its label is {} - It's start is {}, End is {} and It's start word index {} + end word index is {} \".format(ent.text, ent.label_, ent.start_char,ent.end_char, ent.start, ent.end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d2ffb897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's have a look at an example where we are required to add a custom NER\n",
    "text4 = \"Suraj to build a github repository for maintenance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f882e3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is Suraj and its label is GPE - It's start is 0, End is 5 and It's start word index 0 + end word index is 1 \n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(text4)\n",
    "for ent in doc3.ents:\n",
    "        print(\"The text is {} and its label is {} - It's start is {}, End is {} and It's start word index {} + end word index is {} \".format(ent.text, ent.label_, ent.start_char,ent.end_char, ent.start, ent.end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e1d2e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's add\n",
    "#Create the EntityRuler\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "\n",
    "#List of Entities and Patterns\n",
    "patterns = [\n",
    "                {\"label\": \"PRODUCT\", \"pattern\": \"github repository\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "668bcd67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is Suraj and its label is GPE - It's start is 0, End is 5 and It's start word index 0 + end word index is 1 \n",
      "The text is github repository and its label is PRODUCT - It's start is 17, End is 34 and It's start word index 4 + end word index is 6 \n"
     ]
    }
   ],
   "source": [
    "# let's find the updated one\n",
    "doc3 = nlp(text4)\n",
    "for ent in doc3.ents:\n",
    "        print(\"The text is {} and its label is {} - It's start is {}, End is {} and It's start word index {} + end word index is {} \".format(ent.text, ent.label_, ent.start_char,ent.end_char, ent.start, ent.end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "21ba0f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let;s check if this works on multiple instances of same unknown word\n",
    "\n",
    "text5 = \"This is a flute and we are looking for an E sharp flute, Can you please check all the flutes in your inventory ?\"\n",
    "doc4 = nlp(text5)\n",
    "for ent in doc4.ents:\n",
    "        print(\"The text is {} and its label is {} - It's start is {}, End is {} and It's start word index {} + end word index is {} \".format(ent.text, ent.label_, ent.start_char,ent.end_char, ent.start, ent.end))\n",
    "\n",
    "# the o/p of the cell came after re-running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c7573edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's add via ruler\n",
    "#Create the EntityRuler\n",
    "\n",
    "#List of Entities and Patterns\n",
    "patterns = [\n",
    "                {\"label\": \"PRODUCT\", \"pattern\": \"flute\"}\n",
    "            ]\n",
    "\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "319e70fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text is flute and its label is PRODUCT - It's start is 10, End is 15 and It's start word index 3 + end word index is 4 \n",
      "The text is flute and its label is PRODUCT - It's start is 50, End is 55 and It's start word index 12 + end word index is 13 \n"
     ]
    }
   ],
   "source": [
    "# let's check\n",
    "doc4 = nlp(text5)\n",
    "for ent in doc4.ents:\n",
    "        print(\"The text is {} and its label is {} - It's start is {}, End is {} and It's start word index {} + end word index is {} \".format(ent.text, ent.label_, ent.start_char,ent.end_char, ent.start, ent.end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "252daf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "m_tool = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52628fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [{'LOWER':'flute'}, {'LOWER':'flutes'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "58cd271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_tool.add('flute',[patterns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "282b74ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "sentence = nlp(u'This is a flute and we are looking for an E sharp flute, Can you please check all the flutes in your inventory ?')\n",
    "matches = m_tool(sentence)\n",
    "print(matches)\n",
    "# let's match\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  \n",
    "    span = sentence[start:end]                   \n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4c81d00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We-We-nsubj-looking\n",
      "agile developers-developers-pobj-for\n",
      "who-who-nsubj-fasttrack\n",
      "project development-development-dobj-fasttrack\n",
      "our organisation-organisation-pobj-in\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'We are looking for agile developers who can fasttrack project development in our organisation')\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text +'-'+ chunk.root.text + '-'+ chunk.root.dep_ +'-'+ chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af563d36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(doc.noun_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d64fa482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4c03c8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">This is visualisation of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NER\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " module that will assist software developers in the domain of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NLP\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in their company on \n",
       "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Earth\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp('This is visualisation of NER module that will assist software developers in the domain of NLP in their company on Earth')\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4d7f5ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">This is visualisation of \n",
       "<mark class=\"entity\" style=\"background: radial-gradient(yellow,cyan); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NER\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " module that will assist software developers in the domain of \n",
       "<mark class=\"entity\" style=\"background: radial-gradient(yellow,cyan); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    NLP\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in their company on \n",
       "<mark class=\"entity\" style=\"background: radial-gradient(pink,blue); padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Earth\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we can even specify colors and effects to displacy\n",
    "colors = {'ORG':'radial-gradient(yellow,cyan)','LOC':'radial-gradient(pink,blue)'}\n",
    "options = {'ents':['ORG','LOC'], 'colors':colors}\n",
    "displacy.render(doc, style='ent', jupyter=True, options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "30535e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Suraj.\n",
      "I am here to illustrate the first demo of sentence segmentation.\n"
     ]
    }
   ],
   "source": [
    "text = \"This is Suraj. I am here to illustrate the first demo of sentence segmentation.\"\n",
    "doc = nlp(text)\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b728b8a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How we do things when no one is looking at it, Often matters; It is because that's what defines our character.\n"
     ]
    }
   ],
   "source": [
    "text2 = \"How we do things when no one is looking at it, Often matters; It is because that's what defines our character.\"\n",
    "doc2 = nlp(text2)\n",
    "sentences2 = []\n",
    "for sent in doc2.sents:\n",
    "    sentences2.append(sent)\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fa3ee3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "\n",
    "#always add the following decorator \n",
    "@Language.component(\"set_rules\")\n",
    "def set_rules(doc):\n",
    "    for token in doc[:-1]:\n",
    "        if token.text == \";\" or token.text == ',':\n",
    "            doc[token.i+1].is_sent_start=True\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b16947fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.set_rules(doc)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add this to existing nlp pipe\n",
    "nlp.add_pipe(\"set_rules\", before='parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dc159847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec',\n",
       " 'tagger',\n",
       " 'set_rules',\n",
       " 'parser',\n",
       " 'attribute_ruler',\n",
       " 'lemmatizer',\n",
       " 'ner',\n",
       " 'entity_ruler']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4f2d494c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How we do things when no one is looking at it,\n",
      "Often matters;\n",
      "It is because that's what defines our character.\n"
     ]
    }
   ],
   "source": [
    "# re-run the doc object creation\n",
    "doc2 = nlp(text2)\n",
    "sentences2_modified = []\n",
    "for sent in doc2.sents:\n",
    "    sentences2_modified.append(sent)\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "476bdcd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences2_modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "61fdaa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "text3 = \"Hi, This is Suraj. \\n We are back with yet another illustration on the topic. \\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f58bf322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next sentence\n",
      "Hi, This is Suraj. \n",
      " We are back with yet another illustration on the topic. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm', exclude=[\"parser\"])\n",
    "    \n",
    "config = {\"punct_chars\": ['\\n']}\n",
    "nlp.add_pipe(\"sentencizer\", config=config)\n",
    "\n",
    "for sent in nlp(text3).sents:\n",
    "    print(\"next sentence\")\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "181aed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's have a look at one such example\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text = [\"Shall we start a youtube channel on it, What do you think about it?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1324bad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['about', 'channel', 'do', 'it', 'on', 'shall', 'start', 'think', 'we', 'what', 'you', 'youtube']\n",
      "{'shall': 5, 'we': 8, 'start': 6, 'youtube': 11, 'channel': 1, 'on': 4, 'it': 3, 'what': 9, 'do': 2, 'you': 10, 'think': 7, 'about': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maximus1/Desktop/home/Desktop/envs/myenv/lib/python3.7/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(text)\n",
    "#summarise\n",
    "print(vectorizer.get_feature_names())\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04081c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 2 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#encoding\n",
    "encoded_vector = vectorizer.transform(text)\n",
    "#print\n",
    "array = encoded_vector.toarray()\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73e77163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "freq_matrix = pd.DataFrame(array, index = text, columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6a55fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>channel</th>\n",
       "      <th>do</th>\n",
       "      <th>it</th>\n",
       "      <th>on</th>\n",
       "      <th>shall</th>\n",
       "      <th>start</th>\n",
       "      <th>think</th>\n",
       "      <th>we</th>\n",
       "      <th>what</th>\n",
       "      <th>you</th>\n",
       "      <th>youtube</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Shall we start a youtube channel on it, What do you think about it?</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    about  channel  do  it  \\\n",
       "Shall we start a youtube channel on it, What do...      1        1   1   2   \n",
       "\n",
       "                                                    on  shall  start  think  \\\n",
       "Shall we start a youtube channel on it, What do...   1      1      1      1   \n",
       "\n",
       "                                                    we  what  you  youtube  \n",
       "Shall we start a youtube channel on it, What do...   1     1    1        1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d31ef25",
   "metadata": {},
   "outputs": [],
   "source": [
    " # let's understand this via 2 examples.\n",
    "text1 = \"The food is cooked by the maid in the kitchen\"\n",
    "text2 = \"The delicacy is cooked by the shef in the kitchen\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3717ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7a3cd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf =TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a075254",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = tfidf.fit_transform([text1,text2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65aad961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.23602594 0.23602594 0.         0.33172622 0.23602594 0.23602594\n",
      "  0.23602594 0.33172622 0.         0.70807782]\n",
      " [0.23602594 0.23602594 0.33172622 0.         0.23602594 0.23602594\n",
      "  0.23602594 0.         0.33172622 0.70807782]]\n"
     ]
    }
   ],
   "source": [
    "vector = tfidf.transform([text1,text2])\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc8ee3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['by' 'cooked' 'delicacy' 'food' 'in' 'is' 'kitchen' 'maid' 'shef' 'the']\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf.get_feature_names_out()\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "46101f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitchen-0.23602594054740156\n",
      "in-0.23602594054740156\n",
      "maid-0.3317262240477849\n",
      "by-0.23602594054740156\n",
      "cooked-0.23602594054740156\n",
      "is-0.23602594054740156\n",
      "food-0.3317262240477849\n",
      "the-0.7080778216422047\n",
      "shef-0.0\n",
      "delicacy-0.0\n",
      "kitchen-0.23602594054740156\n",
      "in-0.23602594054740156\n",
      "by-0.23602594054740156\n",
      "cooked-0.23602594054740156\n",
      "is-0.23602594054740156\n",
      "the-0.7080778216422047\n"
     ]
    }
   ],
   "source": [
    "for col in out.nonzero()[1]:\n",
    "    print(str(feature_names[col]) +\"-\"+ str(out[0,col]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a44ce28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 1.40546511, 1.40546511, 1.        ,\n",
       "       1.        , 1.        , 1.40546511, 1.40546511, 1.        ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# printing the inverse document frequency.\n",
    "tfidf.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9f44b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
